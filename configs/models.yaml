defaults:
  chat_model: llama3
  embedding_model: nomic-embed-text

models:
  - id: ollama:llama3
    name: llama3
    provider: ollama
    base_url: http://ollama:11434
    remote_name: llama3
    # chat|completion|embedding
    modality: chat
    # tiny|small|medium|large|frontier
    capability: medium
    # ultra-low|low|medium|high
    latency: low
    # ultra-low|low|medium|high
    cost: low
    privacy: high
    enabled: true
    temperature: 0.2
    context_window: 8192
    max_output_tokens: 2048
    tags: [local, fast, coding]

  - id: llama3:nomic-embed-text
    name: nomic-embed-text
    provider: llama3
    base_url: http://ollama:11434
    remote_name: nomic-embed-text
    # chat|completion|embedding
    modality: embeddings
    # tiny|small|medium|large|frontier
    capability: medium
    # ultra-low|low|medium|high 
    latency: low
    # ultra-low|low|medium|high
    cost: low
    privacy: high
    enabled: true
    temperature: 0.2
    tags: [embeddings]
    profiles: [local.embeddings]

  - id: vllm:deepseek-coder-v2
    name: deepseek-coder-v2
    provider: vllm
    base_url: http://vllm:8000/v1
    remote_name: deepseek-coder-v2
    modality: chat
    capability: high
    latency: medium
    cost: low
    privacy: high
    enabled: false
    context_window: 128000
    max_output_tokens: 16384
    temperature: 0.2
    tags: [local, fast, coding]

  - id: deepseek:deepseek-chat
    name: deepseek-chat
    provider: deepseek
    base_url: https://api.deepseek.com/v1
    remote_name: deepseek-chat
    api_key_env: DEEPSEEK_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    privacy: low
    enabled: false
    context_window: 128000
    max_output_tokens: 32768
    temperature: 0.3
    tags: [cheap]
    profiles: [local.codegen]

  - id: openai:gpt-4o-mini
    name: gpt-4o-mini
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    privacy: low
    enabled: true
    temperature: 0.2
    context_window: 128000
    max_output_tokens: 16384
    tags: [cheap, fast]

  - id: openai:gpt-4o
    name: gpt-4o
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: gpt-4o
    api_key_env: OPENAI_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    privacy: low
    enabled: true
    context_window: 128000
    max_output_tokens: 16384
    temperature: 0.2
    tags: [cheap, fast]

  - id: openai:gpt-5-nano
    name: gpt-5-nano
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: gpt-5-nano-2025-08-07
    api_key_env: OPENAI_API_KEY
    modality: chat
    capability: frontier
    latency: medium
    cost: high
    temperature: 0.1
    context_window: 400000
    max_output_tokens: 128000
    tags: [frontier, quality, reasoning]
    enabled: true
   
  - id: openai:gpt-5-mini
    name: gpt-5-mini
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: gpt-5-mini-2025-08-07
    modality: chat
    capability: frontier
    latency: medium
    cost: high
    temperature: 0.1
    context_window: 400000
    max_output_tokens: 128000
    tags: [frontier, quality, reasoning]
    enabled: true
    api_key_env: OPENAI_API_KEY

  - id: openai:gpt-5
    name: gpt-5
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: gpt-5
    modality: chat
    capability: frontier
    latency: medium
    cost: high
    temperature: 0.1
    context_window: 400000
    max_output_tokens: 128000
    tags: [frontier, quality, reasoning]
    enabled: true
    api_key_env: OPENAI_API_KEY

  - id: openai:text-embedding-3-small
    name: text-embedding-3-small
    provider: openai
    base_url: https://api.openai.com/v1
    remote_name: text-embedding-3-small
    api_key_env: OPENAI_API_KEY
    modality: embeddings
    capability: high
    latency: medium
    cost: medium
    privacy: low
    enabled: false

  - id: anthropic:claude-haiku-4-5
    name: claude-haiku-4-5
    provider: anthropic
    base_url: https://api.anthropic.com/v1
    remote_name: claude-opus-4-1-20250805
    api_key_env: ANTHROPIC_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    context_window: 200000
    max_output_tokens: 8192
    privacy: low
    enabled: true

  - id: anthropic:claude-sonnet-4-5
    name: claude-sonnet-4-5
    provider: anthropic
    base_url: https://api.anthropic.com/v1
    remote_name: claude-sonnet-4-5-20250929
    api_key_env: ANTHROPIC_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    context_window: 200000
    max_output_tokens: 8192
    privacy: low
    enabled: true


  - id: anthropic:claude-opus-4-1
    name: claude-opus-4-1
    provider: anthropic
    base_url: https://api.anthropic.com/v1
    remote_name: claude-opus-4-1-20250805
    api_key_env: ANTHROPIC_API_KEY
    modality: chat
    capability: high
    latency: medium
    cost: medium
    context_window: 1000000
    max_output_tokens: 8192
    privacy: low
    enabled: true


# Profili (etichette "umane" che puntano a criteri di selezione dei modelli)
profiles:
  plan.fast:
    select:
      any_tags: [reasoning, fast]
      prefer_providers: [openai, openai]
      model: openai:gpt-5-mini # se presente → selezione deterministica
      fallback: [anthropenaiopic:claude-3.7-haiku, ollama:llama3]   # opzionale
      # opzionale: criteri per caso estremo in cui non troviamo i pin/fallback
      select:
        any_tags: [reasoning, fast]

  code.strict:
    model: openai:gpt-5-thinking
    select:
      any_tags: [coding, quality]
      avoid_tags: [cheap]

  chat.cheap:
    model: anthropic:claude-3.7-haiku
    select:
      any_tags: [cheap]

  local.codegen:
    model: ollama:llama3
    select:
      any_tags: [local, coding]

# Routing per task → profilo (fallback se non si passa un hint)
routing:
  spec: plan.fast
  plan: plan.fast
  kit: code.strict
  build: local.codegen
  chat: chat.cheap

# Pesatura opzionale per tie-break quando più modelli sono eleggibili
scoring:
  weights:
    capability: 0.5      # frontier > large > medium > small > tiny
    latency: 0.2         # ultra-low > low > medium > high
    cost: 0.2            # ultra-low > low > medium > high
    quality: 0.1         # +1 se tag quality/frontier, -1 se cheap
